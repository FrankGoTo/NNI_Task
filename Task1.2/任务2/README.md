# NAS实验

## 实验流程：
* 1.将训练代码中设定合适的学习率、优化参数、网络层数、通道数等初始设置。
* 2.在训练代码的model文件中设定网络的**节点拓扑结构**和每个节点待选取的**操作类型**（如池化、深度可分离卷积等）。
* 3.运行`main_cifar10NAS.py`文件，开始神经网络架构搜索，并将每轮生成的网络参数结果保存在`./checkpoints_layer5`中(网络的搜索空间设置为5层)。
* 4.搜索程序结束后，依照生成网络结构参数，重新再数据训练一遍，测试最终的准确率（训练150轮）。
## 实验结果：
#### 图1 搜索空间为5层 最终准确率
![图片路径 ./pic/image001.png](./pic/image001.png "图片路径 ./pic/image001.png")

## 实验总结
* `step`衰减方式和`cosin`衰减方式相比，前者更容易陷入局部最优，导致精确度不在继续提升，如4所示，很多精确度较差trial，精确度前期有提升，但是到25轮之后，精确度曲线变为直线，不再提升。
* 在优化器选择上，`SGD`的最终效果更好，`adam`优化器前期loss下降较快，但最终效果并不是最好。

